{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d8b4eb",
   "metadata": {},
   "source": [
    "# Parameter testing\n",
    "\n",
    "If not already done, please pay attention to the Requirements and Preparation of the file \"Bike.ipynb\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8fffc",
   "metadata": {},
   "source": [
    "## Select parameter and write in file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb28a0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:be.kuleuven.dtai.distance:DTAIDistance C library not available\n",
      "DEBUG:be.kuleuven.dtai.distance:DTAIDistance C library not available\n",
      "DEBUG:be.kuleuven.dtai.distance:DTAIDistance C-OMP library not available\n",
      "DEBUG:be.kuleuven.dtai.distance:cannot import name 'dtw_cc_omp' from partially initialized module 'dtaidistance' (most likely due to a circular import) (/home/ahmet/miniconda3/envs/test/lib/python3.8/site-packages/dtaidistance/__init__.py)\n",
      "DEBUG:be.kuleuven.dtai.distance:DTAIDistance C-Numpy library not available\n",
      "DEBUG:be.kuleuven.dtai.distance:DTAIDistance C-OMP library not available\n",
      "INFO:be.kuleuven.dtai.distance:tqdm library not available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import deformetrica as dfca\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import torch \n",
    "from dtaidistance import dtw_ndim\n",
    "import xlsxwriter\n",
    "\n",
    "#paramter\n",
    "attachmentType = 'current' #'varifold' \n",
    "kernelWidth = 10\n",
    "deformationKernelWidth = 10\n",
    "noiseStd = 0.5\n",
    "numberOfTimePoints = 11\n",
    "template = 'mask_03' #name of file without file extension\n",
    "templateName = template + '.vtk'\n",
    "initialControlPointsName = template + '_initial_control_points.txt'\n",
    "\n",
    "#write file\n",
    "data_path_results = \"results/\"\n",
    "data_path_control_points = \"images/\"\n",
    "output_file = os.path.join(data_path_results, 'results_parameter_testing.txt' )\n",
    "data_base_control_points = os.path.join(data_path_control_points, 'mask_initial_control_points/')\n",
    "os.makedirs(data_path_results, exist_ok=True)\n",
    "\n",
    "f = open(output_file, \"a\")\n",
    "f.write(\"###########################################################\\n\")\n",
    "f.write(\"parameter:\\n\")\n",
    "f.write(\"attachmentType: \" + attachmentType + \"\\n\")\n",
    "f.write(\"kernelWidth: \" + str(kernelWidth) + \"\\n\")\n",
    "f.write(\"deformationKernelWidth: \" + str(deformationKernelWidth) + \"\\n\")\n",
    "f.write(\"noiseStd: \" + str(noiseStd) + \"\\n\")\n",
    "f.write(\"numberOfTimePoints: \" + str(numberOfTimePoints) + \"\\n\")\n",
    "f.write(\"templateName: \" + templateName + \"\\n\")\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557054e",
   "metadata": {},
   "source": [
    "## Run Deformetrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100b128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger has been set to: INFO\n",
      "OMP_NUM_THREADS was not found in environment variables. An automatic value will be set.\n",
      "OMP_NUM_THREADS will be set to 2\n",
      ">> No specified state-file. By default, Deformetrica state will by saved in file: output/deformetrica-state.p.\n",
      ">> Removing the pre-existing state file with same path.\n",
      ">> Reading 200 initial control points from file images/mask_initial_control_points/mask_03_initial_control_points.txt.\n",
      ">> Momenta initialized to zero, for 5 subjects.\n",
      ">> Started estimator: GradientAscent\n",
      "------------------------------------- Iteration: 0 -------------------------------------\n",
      ">> Log-likelihood = -1.654E+05 \t [ attachment = -1.654E+05 ; regularity = 0.000E+00 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.065E-04   and   9.390E+03 \t[ landmark_points ]\n",
      "\t\t3.037E-04   and   3.293E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 1 -------------------------------------\n",
      ">> Log-likelihood = -1.606E+05 \t [ attachment = -1.606E+05 ; regularity = -1.820E+00 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.597E-04   and   9.091E+03 \t[ landmark_points ]\n",
      "\t\t4.556E-04   and   3.221E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 2 -------------------------------------\n",
      ">> Log-likelihood = -1.536E+05 \t [ attachment = -1.536E+05 ; regularity = -1.107E+01 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t2.396E-04   and   8.651E+03 \t[ landmark_points ]\n",
      "\t\t6.834E-04   and   3.115E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 3 -------------------------------------\n",
      ">> Log-likelihood = -1.441E+05 \t [ attachment = -1.440E+05 ; regularity = -3.836E+01 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t3.594E-04   and   8.014E+03 \t[ landmark_points ]\n",
      "\t\t1.025E-03   and   2.962E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 4 -------------------------------------\n",
      ">> Log-likelihood = -1.314E+05 \t [ attachment = -1.313E+05 ; regularity = -1.055E+02 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t5.391E-04   and   7.115E+03 \t[ landmark_points ]\n",
      "\t\t1.538E-03   and   2.743E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 5 -------------------------------------\n",
      ">> Log-likelihood = -1.157E+05 \t [ attachment = -1.155E+05 ; regularity = -2.531E+02 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t8.087E-04   and   5.913E+03 \t[ landmark_points ]\n",
      "\t\t2.306E-03   and   2.445E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 6 -------------------------------------\n",
      ">> Log-likelihood = -9.802E+04 \t [ attachment = -9.747E+04 ; regularity = -5.463E+02 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.213E-03   and   4.459E+03 \t[ landmark_points ]\n",
      "\t\t3.460E-03   and   2.065E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 7 -------------------------------------\n",
      ">> Log-likelihood = -8.036E+04 \t [ attachment = -7.929E+04 ; regularity = -1.065E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.820E-03   and   2.970E+03 \t[ landmark_points ]\n",
      "\t\t5.189E-03   and   1.632E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 8 -------------------------------------\n",
      ">> Log-likelihood = -6.517E+04 \t [ attachment = -6.331E+04 ; regularity = -1.858E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t2.729E-03   and   1.769E+03 \t[ landmark_points ]\n",
      "\t\t7.784E-03   and   1.196E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 9 -------------------------------------\n",
      ">> Log-likelihood = -5.400E+04 \t [ attachment = -5.113E+04 ; regularity = -2.868E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t4.094E-03   and   1.164E+03 \t[ landmark_points ]\n",
      "\t\t1.168E-02   and   8.230E+02 \t[ momenta ]\n",
      "------------------------------------- Iteration: 10 -------------------------------------\n",
      ">> Log-likelihood = -4.586E+04 \t [ attachment = -4.195E+04 ; regularity = -3.901E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t6.141E-03   and   9.499E+02 \t[ landmark_points ]\n",
      "\t\t1.751E-02   and   6.297E+02 \t[ momenta ]\n",
      "------------------------------------- Iteration: 11 -------------------------------------\n",
      ">> Log-likelihood = -3.912E+04 \t [ attachment = -3.413E+04 ; regularity = -4.994E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t9.212E-03   and   8.860E+02 \t[ landmark_points ]\n",
      "\t\t2.627E-02   and   4.851E+02 \t[ momenta ]\n",
      "------------------------------------- Iteration: 12 -------------------------------------\n",
      ">> Log-likelihood = -3.666E+04 \t [ attachment = -3.025E+04 ; regularity = -6.403E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.382E-02   and   2.118E+03 \t[ landmark_points ]\n",
      "\t\t3.941E-02   and   9.534E+02 \t[ momenta ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t6.909E-03   and   2.118E+03 \t[ landmark_points ]\n",
      "\t\t1.970E-02   and   9.534E+02 \t[ momenta ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t3.454E-03   and   2.118E+03 \t[ landmark_points ]\n",
      "\t\t9.852E-03   and   9.534E+02 \t[ momenta ]\n",
      "------------------------------------- Iteration: 13 -------------------------------------\n",
      ">> Log-likelihood = -3.417E+04 \t [ attachment = -2.772E+04 ; regularity = -6.441E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t5.182E-03   and   2.259E+03 \t[ landmark_points ]\n",
      "\t\t1.478E-02   and   7.955E+02 \t[ momenta ]\n"
     ]
    }
   ],
   "source": [
    "data_path_images = \"images/\"\n",
    "data_base = os.path.join(data_path_images, 'mask_vtk/')\n",
    "\n",
    "iteration_status_dictionaries = []\n",
    "def estimator_callback(status_dict):\n",
    "    iteration_status_dictionaries.append(status_dict)\n",
    "    return True\n",
    "\n",
    "# instantiate a Deformetrica object\n",
    "deformetrica = dfca.Deformetrica(output_dir='output', verbosity='INFO')\n",
    "\n",
    "dataset_specifications = {\n",
    "        'dataset_filenames': [\n",
    "            [{'bike': os.path.join(data_base, 'mask_01.vtk')}],\n",
    "            [{'bike': os.path.join(data_base, 'mask_02.vtk')}],\n",
    "            [{'bike': os.path.join(data_base, 'mask_03.vtk')}],\n",
    "            [{'bike': os.path.join(data_base, 'mask_04.vtk')}],\n",
    "            [{'bike': os.path.join(data_base, 'mask_05.vtk')}]],\n",
    "        'subject_ids': ['mask_01', 'mask_02', 'mask_03', 'mask_04', 'mask_05'],\n",
    "    }\n",
    "template_specifications = {\n",
    "        'bike': {'deformable_object_type': 'polyline',\n",
    "                  'kernel_type': 'torch', 'kernel_width': kernelWidth,\n",
    "                  'noise_std': noiseStd,\n",
    "                  'filename': os.path.join(data_base, templateName),\n",
    "                  'attachment_type': attachmentType}\n",
    "    }\n",
    "estimator_options={'optimization_method_type': 'GradientAscent', 'initial_step_size': 1.,\n",
    "                               'max_iterations': 100, 'max_line_search_iterations': 10, 'callback': estimator_callback}\n",
    "\n",
    "# perform a deterministic atlas estimation\n",
    "model = deformetrica.estimate_deterministic_atlas(template_specifications, dataset_specifications,\n",
    "                                                    estimator_options=estimator_options,\n",
    "                                                    model_options={'deformation_kernel_type': 'torch', 'deformation_kernel_width': deformationKernelWidth, \n",
    "                                                                               'dtype': 'float32', 'number_of_time_points': numberOfTimePoints, \n",
    "                                                                               'initial_control_points': os.path.join(data_base_control_points, initialControlPointsName)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5fac5",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "fixed_effects = model.fixed_effects\n",
    "momenta__est = fixed_effects['momenta']\n",
    "\n",
    "#calculate strength of momenta and return listDescription\n",
    "list_of_momenta =  [[] for i in range(len(momenta__est[0]))]\n",
    "for i, m in enumerate(momenta__est):\n",
    "    for j in range(len(m)):\n",
    "        list_of_momenta[j].append(abs(sqrt(m[j,0] ** 2 + m[j,1] ** 2)))\n",
    "\n",
    "standard_deviation_momenta = [pd.DataFrame(i).describe()[0]['std'] for i in list_of_momenta]\n",
    "listDescription = pd.DataFrame(standard_deviation_momenta).describe()\n",
    "f.write(\"list of momenta standard deviation: \\n\")\n",
    "for i in listDescription[0].keys():\n",
    "    f.write(i + \": \" + str(listDescription[0][i]) + \"\\n\")\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.write(\"dynamic time warping distance between raw and reconstruct: \\n\")\n",
    "sumOfD = 0\n",
    "#calculate distance between raw and reconstruct\n",
    "for i, momentum__est in enumerate(momenta__est):   \n",
    "    path_to_target__raw = dataset_specifications['dataset_filenames'][i][0]['bike']\n",
    "    target_id = dataset_specifications['subject_ids'][i]\n",
    "    target_points__raw, _, target_connectivity__raw = dfca.io.DeformableObjectReader.read_file(path_to_target__raw, extract_connectivity=True)\n",
    "\n",
    "    model.exponential.set_initial_momenta(torch.from_numpy(momentum__est).to(torch.float32))\n",
    "    model.exponential.update()\n",
    "    target_points__rec = model.exponential.get_template_points()['landmark_points'].detach().cpu().numpy()\n",
    "\n",
    "    d = dtw_ndim.distance(target_points__raw, target_points__rec)\n",
    "    sumOfD += d\n",
    "    f.write(str(i+1) + \": \" + str(d) + \"\\n\")\n",
    "f.write(\"average of distance: \" + str(sumOfD/5) + \"\\n\")\n",
    "f.write(\"\\n\")\n",
    "f.close()\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ece805",
   "metadata": {},
   "source": [
    "## Convert text file with results to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929fed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "xlsx_file = os.path.join(data_path_results, 'results_parameter_testing.xlsx') \n",
    "\n",
    "with open(xlsx_file, \"a+\") as c:\n",
    "    print(\"file created\")\n",
    "    \n",
    "workbook = xlsxwriter.Workbook(xlsx_file)\n",
    "worksheet = workbook.add_worksheet(\"results\")\n",
    "data = open(output_file, \"r\")\n",
    "linelist = data.readlines()\n",
    "\n",
    "listOfTableDescription  = [' ', 'parameter:', 'attachmentType:', 'kernelWidth:',\n",
    "                            'deformationKernelWidth:', 'noiseStd:', \n",
    "                            'numberOfTimePoints:', 'templateName:', ' ', \n",
    "                            'list of momenta standard deviation:', 'count:', 'mean:', \n",
    "                            'std:', 'min:', '25%: ', '50%:','75%:', 'max:', ' ', \n",
    "                            'dynamic time warping distance between raw and reconstruct:',\n",
    "                             '1:', '2:', '3:', '4:', '5:','average of distance:']\n",
    "\n",
    "for i in range(len(listOfTableDescription)):\n",
    "    worksheet.write_row(i, 0, [listOfTableDescription[i]])\n",
    "\n",
    "col = 0\n",
    "row = 0\n",
    "for num in range (0, len(linelist)):\n",
    "    line = linelist[num]\n",
    "    if (line[0] == \"#\"):\n",
    "        col += 1\n",
    "        row = 0\n",
    "        continue\n",
    "    splitline = line.split(\"\\t\")\n",
    "    writeLine = [splitline[0][splitline[0].find(\":\") + 1:]]\n",
    "    row += 1\n",
    "    worksheet.write_row(row, col, writeLine) \n",
    "\n",
    "workbook.close() \n",
    "print(\"file closed\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
